{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ga642381/ML2021-Spring/blob/main/HW02/HW02-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYlaRwNu7ojq"
      },
      "source": [
        "# **作业 2-1 音素分类**\n",
        "\n",
        "* 幻灯片: https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/hw/HW02/HW02.pdf\n",
        "* 视频 (中文): https://youtu.be/PdjXnQbu2zo\n",
        "* 视频 (英文): https://youtu.be/ESRr-VCykBs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emUd7uS7crTz"
      },
      "source": [
        "## DARPA TIMIT 声学-音素连续语音语料库 (TIMIT)\n",
        "TIMIT阅读语音语料库的设计目的是提供语音数据，用于获取声学音素知识以及开发和评估自动语音识别系统。\n",
        "\n",
        "本作业是一个多类别分类任务，\n",
        "我们将训练一个深度神经网络分类器，从TIMIT语音语料库中预测每个帧的音素。\n",
        "\n",
        "链接: https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVUGfWTo7_Oj"
      },
      "source": [
        "## 下载数据\n",
        "从谷歌云盘下载数据，然后解压。\n",
        "\n",
        "运行此代码块后，您应该有 `timit_11/train_11.npy`、`timit_11/train_label_11.npy` 和 `timit_11/test_11.npy`。<br><br>\n",
        "`timit_11/`\n",
        "- `train_11.npy`: 训练数据<br>\n",
        "- `train_label_11.npy`: 训练标签<br>\n",
        "- `test_11.npy`: 测试数据<br><br>\n",
        "\n",
        "**注意：如果谷歌云盘链接失效，您可以直接从Kaggle下载数据并上传到工作区**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzkiMEcC3Foq",
        "outputId": "4308c64c-6885-4d1c-8eb7-a2d9b8038401"
      },
      "outputs": [],
      "source": [
        "# !gdown --id '1HPkcmQmFGu-3OknddKIa5dNDsR05lIQR' --output data.zip\n",
        "# !unzip data.zip\n",
        "# !ls "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L_4anls8Drv"
      },
      "source": [
        "## 准备数据\n",
        "从`.npy`文件（NumPy数组）中加载训练和测试数据。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJjLT8em-y9G",
        "outputId": "8edc6bfe-7511-447f-f239-00b96dba6dcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在加载数据...\n",
            "原始训练数据大小: (1229932, 429)\n",
            "原始测试数据大小: (451552, 429)\n",
            "正在选择最重要的 200 个特征...\n",
            "分数最高的20个特征:\n",
            "       特征名称            分数\n",
            "235  特征_235  83663.352355\n",
            "274  特征_274  71385.954938\n",
            "196  特征_196  59692.624024\n",
            "313  特征_313  46536.203068\n",
            "157  特征_157  36152.184568\n",
            "..      ...           ...\n",
            "208  特征_208   3047.407233\n",
            "305  特征_305   2996.665994\n",
            "139  特征_139   2975.303450\n",
            "61    特征_61   2946.420719\n",
            "266  特征_266   2927.731564\n",
            "\n",
            "[200 rows x 2 columns]\n",
            "选定的特征索引: [ 0  1  2  3  4 13 14 15 16 17]... (共200个)\n",
            "特征选择后训练数据大小: (1229932, 200)\n",
            "特征选择后测试数据大小: (451552, 200)\n",
            "正在进行数据归一化...\n",
            "归一化完成\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "print('正在加载数据...')\n",
        "\n",
        "data_root='./timit_11/'\n",
        "train = np.load(data_root + 'train_11.npy')\n",
        "train_label = np.load(data_root + 'train_label_11.npy')\n",
        "test = np.load(data_root + 'test_11.npy')\n",
        "\n",
        "print('原始训练数据大小: {}'.format(train.shape))\n",
        "print('原始测试数据大小: {}'.format(test.shape))\n",
        "USE_FEATURE_SELECTION=True\n",
        "USE_NORMALIZATION=True\n",
        "N_FEATURES=200\n",
        "# 特征选择 - 使用ANOVA F-值找出最重要的特征\n",
        "if USE_FEATURE_SELECTION:\n",
        "    print(f'正在选择最重要的 {N_FEATURES} 个特征...')\n",
        "    selector = SelectKBest(f_classif, k=N_FEATURES)\n",
        "    # 使用所有训练数据进行特征选择\n",
        "    train_new = selector.fit_transform(train, train_label)\n",
        "    test_new = selector.transform(test)\n",
        "    # 打印特征选择的分数\n",
        "    import pandas as pd\n",
        "    \n",
        "    # 获取特征分数\n",
        "    feature_scores = selector.scores_\n",
        "    \n",
        "    # 创建特征索引的DataFrame\n",
        "    df_columns = pd.DataFrame([f\"特征_{i}\" for i in range(train.shape[1])])\n",
        "    df_scores = pd.DataFrame(feature_scores)\n",
        "    \n",
        "    # 合并两个DataFrame以便更好地可视化\n",
        "    feature_scores_df = pd.concat([df_columns, df_scores], axis=1)\n",
        "    feature_scores_df.columns = ['特征名称', '分数']\n",
        "    \n",
        "    # 打印分数最高的20个特征\n",
        "    print(\"分数最高的20个特征:\")\n",
        "    print(feature_scores_df.nlargest(200, '分数'))\n",
        "    # 获取被选中的特征索引\n",
        "    selected_indices = selector.get_support(indices=True)\n",
        "    print(f'选定的特征索引: {selected_indices[:10]}... (共{len(selected_indices)}个)')\n",
        "    \n",
        "    # 更新训练和测试数据\n",
        "    train = train_new\n",
        "    test = test_new\n",
        "    print(f'特征选择后训练数据大小: {train.shape}')\n",
        "    print(f'特征选择后测试数据大小: {test.shape}')\n",
        "\n",
        "# 归一化处理\n",
        "if USE_NORMALIZATION:\n",
        "    print('正在进行数据归一化...')\n",
        "    scaler = StandardScaler()\n",
        "    train = scaler.fit_transform(train)\n",
        "    test = scaler.transform(test)\n",
        "    print('归一化完成')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# feature_scores_df.nlargest(200, '分数')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 所有配置参数 ====================\n",
        "# 数据相关配置\n",
        "VAL_RATIO = 0.2              # 验证集比例（仅在非交叉验证时使用）\n",
        "BATCH_SIZE = 256             # 批处理大小\n",
        "N_FOLDS = 3                  # 交叉验证折数\n",
        "USE_CV = False               # 是否使用交叉验证\n",
        " \n",
        "# 随机种子配置\n",
        "SEED = 182142                # 随机种子值\n",
        "\n",
        "# 特征选择与预处理配置\n",
        "N_FEATURES = 200             # 选择的特征数量\n",
        "USE_FEATURE_SELECTION = True # 是否进行特征选择\n",
        "USE_NORMALIZATION = True     # 是否进行归一化\n",
        "\n",
        "# 训练相关配置\n",
        "NUM_EPOCH = 200              # 训练轮数\n",
        "LEARNING_RATE = 0.0001       # 学习率\n",
        "WEIGHT_DECAY = 1e-4          # L2正则化系数\n",
        "MODEL_PATH = './model.ckpt'  # 模型保存路径\n",
        "\n",
        "# 学习率调度器配置\n",
        "USE_SCHEDULER = True         # 是否使用学习率调度器\n",
        "SCHEDULER_PATIENCE = 5       # 学习率调度器的耐心值 (几轮不提升就降低)\n",
        "SCHEDULER_FACTOR = 0.1       # 学习率缩放因子 (new_lr = old_lr * factor)\n",
        "\n",
        "# ==================== 应用配置 ====================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us5XW_x6udZQ"
      },
      "source": [
        "## 创建数据集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Fjf5EcmJtf4e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# 这段代码定义了一个名为TIMITDataset的自定义数据集类，继承自PyTorch的Dataset类\n",
        "class TIMITDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        # 初始化方法接收特征数据X和可选的标签数据y\n",
        "        # 将NumPy数组X转换为PyTorch浮点张量\n",
        "        self.data = torch.from_numpy(X).float()\n",
        "        if y is not None:\n",
        "            # 如果提供了标签数据y\n",
        "            # 将y转换为整数类型\n",
        "            y = y.astype(int)\n",
        "            # 然后转换为PyTorch长整型张量\n",
        "            self.label = torch.LongTensor(y)\n",
        "        else:\n",
        "            # 如果没有提供标签数据，则标签设为None（用于测试集）\n",
        "            self.label = None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 获取指定索引的数据项\n",
        "        if self.label is not None:\n",
        "            # 如果有标签，返回(数据,标签)对\n",
        "            return self.data[idx], self.label[idx]\n",
        "        else:\n",
        "            # 如果没有标签，只返回数据\n",
        "            return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        # 返回数据集的大小（样本数量）\n",
        "        return len(self.data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otIC6WhGeh9v"
      },
      "source": [
        "将标记数据分为训练集和验证集，您可以修改变量 `VAL_RATIO` 来更改验证数据的比例。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYqi_lAuvC59",
        "outputId": "13dabe63-4849-47ee-fe04-57427b9d601c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "训练集大小: (983945, 200)\n",
            "验证集大小: (245987, 200)\n"
          ]
        }
      ],
      "source": [
        "# 这段代码的作用是将原始的训练数据划分为训练集和验证集。\n",
        "# 使用统一配置文件中定义的VAL_RATIO参数\n",
        "\n",
        "# 计算训练集的样本数量\n",
        "# train.shape[0] 表示训练数据train的样本数量（即行数）\n",
        "# 这里的 shape[0] 表示样本数量（行数），shape[1] 表示特征数量（列数）\n",
        "# 如果写成 train.shape[1]，得到的是特征的数量，而不是样本数量\n",
        "# 例如，如果 train.shape 是 (1000, 39)，那么 train.shape[0]=1000，train.shape[1]=39\n",
        "# 这里用于切分数据集的应该是样本数量，所以用 shape[0]\n",
        "# 你也可以尝试填入其他数值，比如直接写一个整数（如 800），那就是前800个样本作为训练集\n",
        "\n",
        "percent = int(train.shape[0] * (1 - VAL_RATIO))\n",
        "\n",
        "# 根据计算得到的数量，将数据切分为训练集和验证集\n",
        "# train[:percent] 和 train_label[:percent] 分别是训练集的特征和标签\n",
        "# train[percent:] 和 train_label[percent:] 分别是验证集的特征和标签\n",
        "train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\n",
        "\n",
        "# 打印训练集和验证集的大小，方便检查切分是否正确\n",
        "print('训练集大小: {}'.format(train_x.shape))\n",
        "print('验证集大小: {}'.format(val_x.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbCfclUIgMTX"
      },
      "source": [
        "从数据集创建数据加载器，您可以随意调整这里的变量 `BATCH_SIZE`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RUCbQvqJurYc"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_set = TIMITDataset(train_x, train_y)\n",
        "val_set = TIMITDataset(val_x, val_y)\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #只打乱训练数据\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SY7X0lUgb50"
      },
      "source": [
        "清理不需要的变量以节省内存。<br>\n",
        "\n",
        "**注意：如果您稍后需要使用这些变量，则可以删除此代码块或稍后清理不需要的变量<br>数据大小相当大，所以请注意colab中的内存使用情况**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8rzkGraeYeN",
        "outputId": "dc790996-a43c-4a99-90d4-e7928892a899"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "63"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "del train, train_label, train_x, train_y, val_x, val_y\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRqKNvNZwe3V"
      },
      "source": [
        "## 创建模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYr1ng5fh9pA"
      },
      "source": [
        "定义模型架构，鼓励您更改和试验模型架构。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lbZrwT6Ny0XL"
      },
      "outputs": [],
      "source": [
        "# 这段代码定义了一个用于分类任务的神经网络模型，名为Classifier，继承自PyTorch的nn.Module。\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Classifier, self).__init__()\n",
        "        # 定义神经网络结构\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 1024),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.BatchNorm1d(1024),  # 修正BN层的维度\n",
        "            nn.Dropout(p=0.2),  # 添加Dropout减少过拟合\n",
        "            \n",
        "            nn.Linear(1024, 512),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(p=0.2),\n",
        "            \n",
        "            nn.Linear(512, 128),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(p=0.2),\n",
        "            \n",
        "            nn.Linear(128, 39)  # 输出39个类别\n",
        "        )\n",
        "        # 定义损失函数为交叉熵损失，适用于分类任务\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 前向传播\n",
        "        return self.net(x)\n",
        "\n",
        "    def cal_loss(self, pred, target):\n",
        "        # 计算损失\n",
        "        return self.criterion(pred, target)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRYciXZvPbYh"
      },
      "source": [
        "## 训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "y114Vmm3Ja6o"
      },
      "outputs": [],
      "source": [
        "#检查设备\n",
        "def get_device():\n",
        "  return 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEX-yjHjhGuH"
      },
      "source": [
        "固定随机种子以确保结果可重现。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "88xPiUnm0tAd"
      },
      "outputs": [],
      "source": [
        "# 固定随机种子\n",
        "def same_seeds(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  \n",
        "    np.random.seed(seed)  \n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# help(torch.optim.Adam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbBcBXkSp6RA"
      },
      "source": [
        "您可以随意更改这里的训练参数。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "QTp3ZXg1yO9Y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "设备: cuda\n",
            "已为常规训练启用学习率调度器。\n"
          ]
        }
      ],
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# 固定随机种子以确保结果可重现\n",
        "same_seeds(SEED)\n",
        "\n",
        "# 获取设备\n",
        "device = get_device()\n",
        "print(f'设备: {device}')\n",
        "\n",
        "# 根据错误信息，Classifier类需要input_dim参数\n",
        "# 确保在创建模型实例时提供input_dim参数\n",
        "# 例如：model = Classifier(input_dim=YOUR_INPUT_DIM).to(device)\n",
        "\n",
        "# 创建模型，定义损失函数和优化器\n",
        "# 注意：此处的 train_x.shape[1] 必须在 USE_CV=False 时才有效\n",
        "# 因为在交叉验证模式下，train_x 可能尚未定义\n",
        "if not USE_CV:\n",
        "    model = Classifier(input_dim=train_x.shape[1]).to(device)\n",
        "else:\n",
        "    # 在CV模式下，我们将在循环内创建模型，此处先创建一个占位符\n",
        "    model = Classifier(input_dim=N_FEATURES).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "# 添加L2正则化（weight_decay参数）\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# 初始化学习率调度器\n",
        "scheduler = None\n",
        "if USE_SCHEDULER and not USE_CV:\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=SCHEDULER_FACTOR, patience=SCHEDULER_PATIENCE)\n",
        "    print(\"已为常规训练启用学习率调度器。\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA是否可用: True\n",
            "CUDA设备数量: 1\n",
            "当前CUDA设备索引: 0\n",
            "当前CUDA设备名称: NVIDIA GeForce RTX 4070 Laptop GPU\n",
            "设备总内存: 8.00 GB\n",
            "多处理器数量: 36\n",
            "CUDA能力: 8.9\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# 检查是否有CUDA可用\n",
        "print(f\"CUDA是否可用: {torch.cuda.is_available()}\")\n",
        "\n",
        "# 查看可用的CUDA设备数量\n",
        "print(f\"CUDA设备数量: {torch.cuda.device_count()}\")\n",
        "\n",
        "# 查看当前CUDA设备\n",
        "print(f\"当前CUDA设备索引: {torch.cuda.current_device()}\")\n",
        "\n",
        "# 获取当前设备名称\n",
        "print(f\"当前CUDA设备名称: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
        "\n",
        "# 获取设备属性\n",
        "if torch.cuda.is_available():\n",
        "    device_props = torch.cuda.get_device_properties(torch.cuda.current_device())\n",
        "    print(f\"设备总内存: {device_props.total_memory / 1024**3:.2f} GB\")\n",
        "    print(f\"多处理器数量: {device_props.multi_processor_count}\")\n",
        "    print(f\"CUDA能力: {device_props.major}.{device_props.minor}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 在代码中执行系统命令\n",
        "import os\n",
        "os.system(\"nvidia-smi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "开始进行3折交叉验证...\n",
            "\n",
            "=== 第 1/3 折训练开始 ===\n",
            "[折 1, 轮 1/50] 训练准确率: 0.566922 损失: 1.484077 | 验证准确率: 0.648306 损失: 1.128509\n",
            "保存第 1 折最佳模型，准确率: 0.648306\n",
            "[折 1, 轮 2/50] 训练准确率: 0.629127 损失: 1.195280 | 验证准确率: 0.674697 损失: 1.027675\n",
            "保存第 1 折最佳模型，准确率: 0.674697\n",
            "[折 1, 轮 3/50] 训练准确率: 0.647635 损失: 1.122423 | 验证准确率: 0.686378 损失: 0.979450\n",
            "保存第 1 折最佳模型，准确率: 0.686378\n",
            "[折 1, 轮 4/50] 训练准确率: 0.659736 损失: 1.075178 | 验证准确率: 0.696479 损失: 0.942951\n",
            "保存第 1 折最佳模型，准确率: 0.696479\n",
            "[折 1, 轮 5/50] 训练准确率: 0.669552 损失: 1.040021 | 验证准确率: 0.702262 损失: 0.919054\n",
            "保存第 1 折最佳模型，准确率: 0.702262\n",
            "[折 1, 轮 6/50] 训练准确率: 0.677645 损失: 1.011703 | 验证准确率: 0.708426 损失: 0.897639\n",
            "保存第 1 折最佳模型，准确率: 0.708426\n",
            "[折 1, 轮 7/50] 训练准确率: 0.683079 损失: 0.988557 | 验证准确率: 0.714233 损失: 0.878575\n",
            "保存第 1 折最佳模型，准确率: 0.714233\n",
            "[折 1, 轮 8/50] 训练准确率: 0.688657 损失: 0.969441 | 验证准确率: 0.717551 损失: 0.864509\n",
            "保存第 1 折最佳模型，准确率: 0.717551\n",
            "[折 1, 轮 9/50] 训练准确率: 0.693584 损失: 0.952226 | 验证准确率: 0.721961 损失: 0.850505\n",
            "保存第 1 折最佳模型，准确率: 0.721961\n",
            "[折 1, 轮 10/50] 训练准确率: 0.697372 损失: 0.937330 | 验证准确率: 0.724912 损失: 0.840614\n",
            "保存第 1 折最佳模型，准确率: 0.724912\n",
            "[折 1, 轮 11/50] 训练准确率: 0.701476 损失: 0.924044 | 验证准确率: 0.727939 损失: 0.828809\n",
            "保存第 1 折最佳模型，准确率: 0.727939\n",
            "[折 1, 轮 12/50] 训练准确率: 0.703998 损失: 0.912409 | 验证准确率: 0.729642 损失: 0.822610\n",
            "保存第 1 折最佳模型，准确率: 0.729642\n",
            "[折 1, 轮 13/50] 训练准确率: 0.707893 损失: 0.900483 | 验证准确率: 0.731883 损失: 0.815563\n",
            "保存第 1 折最佳模型，准确率: 0.731883\n",
            "[折 1, 轮 14/50] 训练准确率: 0.710607 损失: 0.890531 | 验证准确率: 0.733588 损失: 0.808783\n",
            "保存第 1 折最佳模型，准确率: 0.733588\n",
            "[折 1, 轮 15/50] 训练准确率: 0.712027 损失: 0.882366 | 验证准确率: 0.735466 损失: 0.800914\n",
            "保存第 1 折最佳模型，准确率: 0.735466\n",
            "[折 1, 轮 16/50] 训练准确率: 0.715343 损失: 0.873891 | 验证准确率: 0.737747 损失: 0.794428\n",
            "保存第 1 折最佳模型，准确率: 0.737747\n",
            "[折 1, 轮 17/50] 训练准确率: 0.716767 损失: 0.866396 | 验证准确率: 0.739337 损失: 0.789870\n",
            "保存第 1 折最佳模型，准确率: 0.739337\n",
            "[折 1, 轮 18/50] 训练准确率: 0.719005 损失: 0.859376 | 验证准确率: 0.740730 损失: 0.785186\n",
            "保存第 1 折最佳模型，准确率: 0.740730\n",
            "[折 1, 轮 19/50] 训练准确率: 0.721699 损失: 0.850890 | 验证准确率: 0.742369 损失: 0.779691\n",
            "保存第 1 折最佳模型，准确率: 0.742369\n",
            "[折 1, 轮 20/50] 训练准确率: 0.722564 损失: 0.845796 | 验证准确率: 0.743386 损失: 0.775031\n",
            "保存第 1 折最佳模型，准确率: 0.743386\n",
            "[折 1, 轮 21/50] 训练准确率: 0.724867 损失: 0.839475 | 验证准确率: 0.744445 损失: 0.772674\n",
            "保存第 1 折最佳模型，准确率: 0.744445\n",
            "[折 1, 轮 22/50] 训练准确率: 0.725942 损失: 0.834558 | 验证准确率: 0.746616 损失: 0.766809\n",
            "保存第 1 折最佳模型，准确率: 0.746616\n",
            "[折 1, 轮 23/50] 训练准确率: 0.727547 损失: 0.829589 | 验证准确率: 0.746491 损失: 0.765207\n",
            "[折 1, 轮 24/50] 训练准确率: 0.729346 损失: 0.823873 | 验证准确率: 0.748101 损失: 0.760992\n",
            "保存第 1 折最佳模型，准确率: 0.748101\n",
            "[折 1, 轮 25/50] 训练准确率: 0.730240 损失: 0.819119 | 验证准确率: 0.748984 损失: 0.757198\n",
            "保存第 1 折最佳模型，准确率: 0.748984\n",
            "[折 1, 轮 26/50] 训练准确率: 0.731920 损失: 0.814287 | 验证准确率: 0.750643 损失: 0.753044\n",
            "保存第 1 折最佳模型，准确率: 0.750643\n",
            "[折 1, 轮 27/50] 训练准确率: 0.732730 损失: 0.810401 | 验证准确率: 0.751077 损失: 0.750768\n",
            "保存第 1 折最佳模型，准确率: 0.751077\n",
            "[折 1, 轮 28/50] 训练准确率: 0.734233 损失: 0.805506 | 验证准确率: 0.751050 损失: 0.749773\n",
            "[折 1, 轮 29/50] 训练准确率: 0.735249 损失: 0.802660 | 验证准确率: 0.752667 损失: 0.746246\n",
            "保存第 1 折最佳模型，准确率: 0.752667\n",
            "[折 1, 轮 30/50] 训练准确率: 0.736296 损失: 0.798335 | 验证准确率: 0.753375 损失: 0.743054\n",
            "保存第 1 折最佳模型，准确率: 0.753375\n",
            "[折 1, 轮 31/50] 训练准确率: 0.736516 损失: 0.795937 | 验证准确率: 0.753760 损失: 0.740775\n",
            "保存第 1 折最佳模型，准确率: 0.753760\n",
            "[折 1, 轮 32/50] 训练准确率: 0.738700 损失: 0.791656 | 验证准确率: 0.755009 损失: 0.738507\n",
            "保存第 1 折最佳模型，准确率: 0.755009\n",
            "[折 1, 轮 33/50] 训练准确率: 0.739172 损失: 0.788627 | 验证准确率: 0.755001 损失: 0.737591\n",
            "[折 1, 轮 34/50] 训练准确率: 0.740576 损失: 0.784694 | 验证准确率: 0.755602 损失: 0.735637\n",
            "保存第 1 折最佳模型，准确率: 0.755602\n",
            "[折 1, 轮 35/50] 训练准确率: 0.740681 损失: 0.782929 | 验证准确率: 0.756636 损失: 0.732038\n",
            "保存第 1 折最佳模型，准确率: 0.756636\n",
            "[折 1, 轮 36/50] 训练准确率: 0.742215 损失: 0.779141 | 验证准确率: 0.757060 损失: 0.730073\n",
            "保存第 1 折最佳模型，准确率: 0.757060\n",
            "[折 1, 轮 37/50] 训练准确率: 0.742978 损失: 0.774637 | 验证准确率: 0.757309 损失: 0.729670\n",
            "保存第 1 折最佳模型，准确率: 0.757309\n",
            "[折 1, 轮 38/50] 训练准确率: 0.742864 损失: 0.775078 | 验证准确率: 0.758228 损失: 0.726462\n",
            "保存第 1 折最佳模型，准确率: 0.758228\n",
            "[折 1, 轮 39/50] 训练准确率: 0.743981 损失: 0.772162 | 验证准确率: 0.758094 损失: 0.726701\n",
            "[折 1, 轮 40/50] 训练准确率: 0.744121 损失: 0.769959 | 验证准确率: 0.759036 损失: 0.722866\n",
            "保存第 1 折最佳模型，准确率: 0.759036\n",
            "[折 1, 轮 41/50] 训练准确率: 0.745238 损失: 0.766680 | 验证准确率: 0.760116 损失: 0.719911\n",
            "保存第 1 折最佳模型，准确率: 0.760116\n",
            "[折 1, 轮 42/50] 训练准确率: 0.746122 损失: 0.763701 | 验证准确率: 0.760146 损失: 0.721103\n",
            "保存第 1 折最佳模型，准确率: 0.760146\n",
            "[折 1, 轮 43/50] 训练准确率: 0.746637 损失: 0.762287 | 验证准确率: 0.760424 损失: 0.719468\n",
            "保存第 1 折最佳模型，准确率: 0.760424\n",
            "[折 1, 轮 44/50] 训练准确率: 0.747522 损失: 0.759129 | 验证准确率: 0.761492 损失: 0.716574\n",
            "保存第 1 折最佳模型，准确率: 0.761492\n",
            "[折 1, 轮 45/50] 训练准确率: 0.747032 损失: 0.758210 | 验证准确率: 0.760663 损失: 0.717723\n",
            "[折 1, 轮 46/50] 训练准确率: 0.748154 损失: 0.756010 | 验证准确率: 0.761226 损失: 0.715630\n",
            "[折 1, 轮 47/50] 训练准确率: 0.749612 损失: 0.753028 | 验证准确率: 0.762826 损失: 0.712804\n",
            "保存第 1 折最佳模型，准确率: 0.762826\n",
            "[折 1, 轮 48/50] 训练准确率: 0.749190 损失: 0.751955 | 验证准确率: 0.762141 损失: 0.712806\n",
            "[折 1, 轮 49/50] 训练准确率: 0.749121 损失: 0.750086 | 验证准确率: 0.763424 损失: 0.709136\n",
            "保存第 1 折最佳模型，准确率: 0.763424\n",
            "[折 1, 轮 50/50] 训练准确率: 0.750671 损失: 0.747618 | 验证准确率: 0.763731 损失: 0.709598\n",
            "保存第 1 折最佳模型，准确率: 0.763731\n",
            "第 1 折最佳验证准确率: 0.763731\n",
            "\n",
            "=== 第 2/3 折训练开始 ===\n",
            "[折 2, 轮 1/50] 训练准确率: 0.567878 损失: 1.478178 | 验证准确率: 0.650234 损失: 1.128080\n",
            "保存第 2 折最佳模型，准确率: 0.650234\n",
            "[折 2, 轮 2/50] 训练准确率: 0.629320 损失: 1.194234 | 验证准确率: 0.672340 损失: 1.033487\n",
            "保存第 2 折最佳模型，准确率: 0.672340\n",
            "[折 2, 轮 3/50] 训练准确率: 0.648055 损失: 1.119617 | 验证准确率: 0.686619 损失: 0.981703\n",
            "保存第 2 折最佳模型，准确率: 0.686619\n",
            "[折 2, 轮 4/50] 训练准确率: 0.659839 损失: 1.074306 | 验证准确率: 0.696507 损失: 0.945879\n",
            "保存第 2 折最佳模型，准确率: 0.696507\n",
            "[折 2, 轮 5/50] 训练准确率: 0.669462 损失: 1.038766 | 验证准确率: 0.702925 损失: 0.920505\n",
            "保存第 2 折最佳模型，准确率: 0.702925\n",
            "[折 2, 轮 6/50] 训练准确率: 0.676470 损失: 1.010752 | 验证准确率: 0.709645 损失: 0.896800\n",
            "保存第 2 折最佳模型，准确率: 0.709645\n",
            "[折 2, 轮 7/50] 训练准确率: 0.683812 损失: 0.987111 | 验证准确率: 0.714425 损失: 0.879567\n",
            "保存第 2 折最佳模型，准确率: 0.714425\n",
            "[折 2, 轮 8/50] 训练准确率: 0.688505 损失: 0.967468 | 验证准确率: 0.718370 损失: 0.864938\n",
            "保存第 2 折最佳模型，准确率: 0.718370\n",
            "[折 2, 轮 9/50] 训练准确率: 0.694071 损失: 0.950654 | 验证准确率: 0.721499 损失: 0.854515\n",
            "保存第 2 折最佳模型，准确率: 0.721499\n",
            "[折 2, 轮 10/50] 训练准确率: 0.697638 损失: 0.935976 | 验证准确率: 0.725160 损失: 0.841139\n",
            "保存第 2 折最佳模型，准确率: 0.725160\n",
            "[折 2, 轮 11/50] 训练准确率: 0.701665 损失: 0.922513 | 验证准确率: 0.728212 损失: 0.831015\n",
            "保存第 2 折最佳模型，准确率: 0.728212\n",
            "[折 2, 轮 12/50] 训练准确率: 0.704506 损失: 0.910570 | 验证准确率: 0.730375 损失: 0.823589\n",
            "保存第 2 折最佳模型，准确率: 0.730375\n",
            "[折 2, 轮 13/50] 训练准确率: 0.708346 损失: 0.899408 | 验证准确率: 0.732966 损失: 0.813726\n",
            "保存第 2 折最佳模型，准确率: 0.732966\n",
            "[折 2, 轮 14/50] 训练准确率: 0.710367 损失: 0.890858 | 验证准确率: 0.734139 损失: 0.808772\n",
            "保存第 2 折最佳模型，准确率: 0.734139\n",
            "[折 2, 轮 15/50] 训练准确率: 0.713120 损失: 0.881027 | 验证准确率: 0.736736 损失: 0.800594\n",
            "保存第 2 折最佳模型，准确率: 0.736736\n",
            "[折 2, 轮 16/50] 训练准确率: 0.715392 损失: 0.872663 | 验证准确率: 0.738254 损失: 0.796330\n",
            "保存第 2 折最佳模型，准确率: 0.738254\n",
            "[折 2, 轮 17/50] 训练准确率: 0.718114 损失: 0.864403 | 验证准确率: 0.740168 损失: 0.788545\n",
            "保存第 2 折最佳模型，准确率: 0.740168\n",
            "[折 2, 轮 18/50] 训练准确率: 0.720071 损失: 0.857268 | 验证准确率: 0.741295 损失: 0.784906\n",
            "保存第 2 折最佳模型，准确率: 0.741295\n",
            "[折 2, 轮 19/50] 训练准确率: 0.721830 损失: 0.849709 | 验证准确率: 0.743042 损失: 0.780340\n",
            "保存第 2 折最佳模型，准确率: 0.743042\n",
            "[折 2, 轮 20/50] 训练准确率: 0.723540 损失: 0.843952 | 验证准确率: 0.744056 损失: 0.775904\n",
            "保存第 2 折最佳模型，准确率: 0.744056\n",
            "[折 2, 轮 21/50] 训练准确率: 0.725386 损失: 0.836950 | 验证准确率: 0.745134 损失: 0.771676\n",
            "保存第 2 折最佳模型，准确率: 0.745134\n",
            "[折 2, 轮 22/50] 训练准确率: 0.726337 损失: 0.832011 | 验证准确率: 0.746200 损失: 0.768539\n",
            "保存第 2 折最佳模型，准确率: 0.746200\n",
            "[折 2, 轮 23/50] 训练准确率: 0.727820 损失: 0.826857 | 验证准确率: 0.746749 损失: 0.764564\n",
            "保存第 2 折最佳模型，准确率: 0.746749\n",
            "[折 2, 轮 24/50] 训练准确率: 0.729434 损失: 0.822224 | 验证准确率: 0.748925 损失: 0.758938\n",
            "保存第 2 折最佳模型，准确率: 0.748925\n",
            "[折 2, 轮 25/50] 训练准确率: 0.730274 损失: 0.818508 | 验证准确率: 0.749737 损失: 0.755387\n",
            "保存第 2 折最佳模型，准确率: 0.749737\n",
            "[折 2, 轮 26/50] 训练准确率: 0.731371 损失: 0.814795 | 验证准确率: 0.751067 损失: 0.753023\n",
            "保存第 2 折最佳模型，准确率: 0.751067\n",
            "[折 2, 轮 27/50] 训练准确率: 0.733154 损失: 0.808607 | 验证准确率: 0.752120 损失: 0.749594\n",
            "保存第 2 折最佳模型，准确率: 0.752120\n",
            "[折 2, 轮 28/50] 训练准确率: 0.734215 损失: 0.805061 | 验证准确率: 0.752579 损失: 0.746620\n",
            "保存第 2 折最佳模型，准确率: 0.752579\n",
            "[折 2, 轮 29/50] 训练准确率: 0.735595 损失: 0.801270 | 验证准确率: 0.753318 损失: 0.745018\n",
            "保存第 2 折最佳模型，准确率: 0.753318\n",
            "[折 2, 轮 30/50] 训练准确率: 0.736215 损失: 0.797358 | 验证准确率: 0.753396 损失: 0.744049\n",
            "保存第 2 折最佳模型，准确率: 0.753396\n",
            "[折 2, 轮 31/50] 训练准确率: 0.737776 损失: 0.793714 | 验证准确率: 0.753737 损失: 0.741782\n",
            "保存第 2 折最佳模型，准确率: 0.753737\n",
            "[折 2, 轮 32/50] 训练准确率: 0.738447 损失: 0.789436 | 验证准确率: 0.756028 损失: 0.739298\n",
            "保存第 2 折最佳模型，准确率: 0.756028\n",
            "[折 2, 轮 33/50] 训练准确率: 0.739314 损失: 0.787064 | 验证准确率: 0.755962 损失: 0.735511\n",
            "[折 2, 轮 34/50] 训练准确率: 0.740646 损失: 0.783904 | 验证准确率: 0.755667 损失: 0.734675\n",
            "[折 2, 轮 35/50] 训练准确率: 0.740730 损失: 0.780680 | 验证准确率: 0.757311 损失: 0.731299\n",
            "保存第 2 折最佳模型，准确率: 0.757311\n",
            "[折 2, 轮 36/50] 训练准确率: 0.741983 损失: 0.777483 | 验证准确率: 0.757679 损失: 0.729606\n",
            "保存第 2 折最佳模型，准确率: 0.757679\n",
            "[折 2, 轮 37/50] 训练准确率: 0.742474 损失: 0.775747 | 验证准确率: 0.758586 损失: 0.727431\n",
            "保存第 2 折最佳模型，准确率: 0.758586\n",
            "[折 2, 轮 38/50] 训练准确率: 0.743644 损失: 0.773498 | 验证准确率: 0.757957 损失: 0.725997\n",
            "[折 2, 轮 39/50] 训练准确率: 0.744131 损失: 0.769968 | 验证准确率: 0.759026 损失: 0.724514\n",
            "保存第 2 折最佳模型，准确率: 0.759026\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[21], line 76\u001b[0m\n\u001b[0;32m     73\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     75\u001b[0m fold_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 76\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfold_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m fold_criterion(outputs, labels)\n\u001b[0;32m     78\u001b[0m batch_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[1;32me:\\anaconda\\envs\\learntorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32me:\\anaconda\\envs\\learntorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[1;32mIn[13], line 32\u001b[0m, in \u001b[0;36mClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32me:\\anaconda\\envs\\learntorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32me:\\anaconda\\envs\\learntorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32me:\\anaconda\\envs\\learntorch\\lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32me:\\anaconda\\envs\\learntorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32me:\\anaconda\\envs\\learntorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32me:\\anaconda\\envs\\learntorch\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:70\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32me:\\anaconda\\envs\\learntorch\\lib\\site-packages\\torch\\nn\\functional.py:1425\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m-> 1425\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1426\u001b[0m )\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 进行3折交叉验证\n",
        "from sklearn.model_selection import KFold\n",
        "import copy\n",
        "import time\n",
        "\n",
        "# 只有在启用交叉验证时才执行\n",
        "if USE_CV:\n",
        "    print(\"开始进行3折交叉验证...\")\n",
        "    \n",
        "    # 重新加载全部训练数据（若之前已删除）\n",
        "    data_root='./timit_11/'\n",
        "    if 'train' not in locals() or 'train_label' not in locals():\n",
        "        print(\"重新加载训练数据...\")\n",
        "        train = np.load(data_root + 'train_11.npy')\n",
        "        train_label = np.load(data_root + 'train_label_11.npy')\n",
        "    \n",
        "    # 应用特征选择和归一化\n",
        "    if USE_FEATURE_SELECTION and 'selector' not in locals():\n",
        "        print(f'正在选择最重要的 {N_FEATURES} 个特征...')\n",
        "        selector = SelectKBest(f_classif, k=N_FEATURES)\n",
        "        train = selector.fit_transform(train, train_label)\n",
        "        test = selector.transform(test)\n",
        "        print(f'特征选择后数据大小: {train.shape}')\n",
        "        \n",
        "    if USE_NORMALIZATION and 'scaler' not in locals():\n",
        "        print('正在进行数据归一化...')\n",
        "        scaler = StandardScaler()\n",
        "        train = scaler.fit_transform(train)\n",
        "        test = scaler.transform(test)\n",
        "        print('归一化完成')\n",
        "    \n",
        "    # 初始化K折交叉验证\n",
        "    kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "    fold_val_accs = []\n",
        "    \n",
        "    # 早停参数\n",
        "    PATIENCE = 5  # 连续10轮验证集性能不提升则停止\n",
        "    \n",
        "    # 开始K折循环\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(train)):\n",
        "        print(f\"\\n=== 第 {fold+1}/{N_FOLDS} 折训练开始 ===\")\n",
        "        \n",
        "        # 准备当前折的数据\n",
        "        train_x, train_y = train[train_idx], train_label[train_idx]\n",
        "        val_x, val_y = train[val_idx], train_label[val_idx]\n",
        "        \n",
        "        # 创建数据加载器\n",
        "        train_set = TIMITDataset(train_x, train_y)\n",
        "        val_set = TIMITDataset(val_x, val_y)\n",
        "        train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
        "        \n",
        "        # 创建新模型实例\n",
        "        fold_model = Classifier(input_dim=train_x.shape[1]).to(device)\n",
        "        fold_criterion = nn.CrossEntropyLoss()\n",
        "        fold_optimizer = torch.optim.Adam(fold_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "        \n",
        "        # 训练当前折\n",
        "        best_val_acc = 0.0\n",
        "        fold_path = f'./model_fold_{fold+1}.ckpt'\n",
        "        \n",
        "        # 早停计数器\n",
        "        patience_counter = 0\n",
        "        \n",
        "        for epoch in range(NUM_EPOCH):\n",
        "            # 训练阶段\n",
        "            fold_model.train()\n",
        "            train_loss = 0.0\n",
        "            train_acc = 0.0\n",
        "            \n",
        "            for i, data in enumerate(train_loader):\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                \n",
        "                fold_optimizer.zero_grad()\n",
        "                outputs = fold_model(inputs)\n",
        "                batch_loss = fold_criterion(outputs, labels)\n",
        "                batch_loss.backward()\n",
        "                fold_optimizer.step()\n",
        "                \n",
        "                _, train_pred = torch.max(outputs, 1)\n",
        "                train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
        "                train_loss += batch_loss.item()\n",
        "            \n",
        "            # 验证阶段\n",
        "            fold_model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_acc = 0.0\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                for i, data in enumerate(val_loader):\n",
        "                    inputs, labels = data\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    \n",
        "                    outputs = fold_model(inputs)\n",
        "                    batch_loss = fold_criterion(outputs, labels)\n",
        "                    \n",
        "                    _, val_pred = torch.max(outputs, 1)\n",
        "                    val_acc += (val_pred.cpu() == labels.cpu()).sum().item()\n",
        "                    val_loss += batch_loss.item()\n",
        "            \n",
        "            # 计算准确率\n",
        "            train_acc /= len(train_set)\n",
        "            val_acc /= len(val_set)\n",
        "            train_loss /= len(train_loader)\n",
        "            val_loss /= len(val_loader)\n",
        "            \n",
        "            # 输出当前训练状态\n",
        "            print(f'[折 {fold+1}, 轮 {epoch+1}/{NUM_EPOCH}] 训练准确率: {train_acc:.6f} 损失: {train_loss:.6f} | 验证准确率: {val_acc:.6f} 损失: {val_loss:.6f}')\n",
        "            \n",
        "            # 保存最佳模型\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                torch.save(fold_model.state_dict(), fold_path)\n",
        "                print(f'保存第 {fold+1} 折最佳模型，准确率: {best_val_acc:.6f}')\n",
        "                patience_counter = 0  # 重置早停计数器\n",
        "            else:\n",
        "                patience_counter += 1  # 增加早停计数器\n",
        "                \n",
        "            # 早停检查\n",
        "            if patience_counter >= PATIENCE:\n",
        "                print(f'验证准确率连续 {PATIENCE} 轮未提升，提前停止训练')\n",
        "                break\n",
        "        \n",
        "        # 记录当前折的最佳验证准确率\n",
        "        fold_val_accs.append(best_val_acc)\n",
        "        print(f\"第 {fold+1} 折最佳验证准确率: {best_val_acc:.6f}\")\n",
        "    \n",
        "    # 输出交叉验证结果\n",
        "    cv_avg = sum(fold_val_accs) / len(fold_val_accs)\n",
        "    print(f\"\\n3折交叉验证平均准确率: {cv_avg:.6f}\")\n",
        "    print(f\"各折验证准确率: {fold_val_accs}\")\n",
        "    \n",
        "    # 在全部训练数据上训练最终模型\n",
        "    print(\"\\n在全部训练数据上训练最终模型...\")\n",
        "    \n",
        "    # 准备全部训练数据\n",
        "    full_train_set = TIMITDataset(train, train_label)\n",
        "    full_train_loader = DataLoader(full_train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    \n",
        "    # 创建最终模型\n",
        "    final_model = Classifier().to(device)\n",
        "    final_criterion = nn.CrossEntropyLoss()\n",
        "    final_optimizer = torch.optim.Adam(final_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    \n",
        "    # 训练最终模型（使用早停）\n",
        "    best_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "    \n",
        "    for epoch in range(NUM_EPOCH):\n",
        "        final_model.train()\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "        \n",
        "        for i, data in enumerate(full_train_loader):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            \n",
        "            final_optimizer.zero_grad()\n",
        "            outputs = final_model(inputs)\n",
        "            batch_loss = final_criterion(outputs, labels)\n",
        "            batch_loss.backward()\n",
        "            final_optimizer.step()\n",
        "            \n",
        "            _, train_pred = torch.max(outputs, 1)\n",
        "            train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
        "            train_loss += batch_loss.item()\n",
        "        \n",
        "        # 计算训练准确率\n",
        "        train_acc /= len(full_train_set)\n",
        "        train_loss /= len(full_train_loader)\n",
        "        \n",
        "        # 输出训练状态\n",
        "        print(f'[最终模型 轮 {epoch+1}/{NUM_EPOCH}] 训练准确率: {train_acc:.6f} 损失: {train_loss:.6f}')\n",
        "        \n",
        "        # 保存最佳模型（基于训练准确率，因为没有验证集）\n",
        "        if train_acc > best_acc:\n",
        "            best_acc = train_acc\n",
        "            best_model_state = copy.deepcopy(final_model.state_dict())\n",
        "            print(f'保存最佳模型，准确率: {best_acc:.6f}')\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            \n",
        "        # 早停检查\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(f'训练准确率连续 {PATIENCE} 轮未提升，提前停止训练')\n",
        "            break\n",
        "    \n",
        "    # 加载最佳模型状态\n",
        "    final_model.load_state_dict(best_model_state)\n",
        "    \n",
        "    # 保存最终模型\n",
        "    torch.save(final_model.state_dict(), MODEL_PATH)\n",
        "    print(f\"最终模型已保存到 {MODEL_PATH}\")\n",
        "    \n",
        "    # 使用最终模型进行预测\n",
        "    model = final_model\n",
        "else:\n",
        "    print(\"跳过交叉验证，使用常规训练...\")\n",
        "    # 这里会执行原来的训练代码"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdMWsBs7zzNs",
        "outputId": "c5ed561e-610d-4a35-d936-fd97adf342a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[001/200] 训练准确率: 0.568413 损失: 1.479155 | 验证准确率: 0.650388 损失: 1.125137\n",
            "保存模型，准确率 0.650\n",
            "[002/200] 训练准确率: 0.629778 损失: 1.191593 | 验证准确率: 0.674567 损失: 1.028039\n",
            "保存模型，准确率 0.675\n",
            "[003/200] 训练准确率: 0.648529 损失: 1.120247 | 验证准确率: 0.686180 损失: 0.983682\n",
            "保存模型，准确率 0.686\n",
            "[004/200] 训练准确率: 0.660057 损失: 1.073676 | 验证准确率: 0.694707 损失: 0.948256\n",
            "保存模型，准确率 0.695\n",
            "[005/200] 训练准确率: 0.669855 损失: 1.038205 | 验证准确率: 0.702532 损失: 0.921561\n",
            "保存模型，准确率 0.703\n",
            "[006/200] 训练准确率: 0.677280 损失: 1.010579 | 验证准确率: 0.708671 损失: 0.899760\n",
            "保存模型，准确率 0.709\n",
            "[007/200] 训练准确率: 0.684220 损失: 0.987131 | 验证准确率: 0.714267 损失: 0.879771\n",
            "保存模型，准确率 0.714\n",
            "[008/200] 训练准确率: 0.688609 损失: 0.969237 | 验证准确率: 0.717557 损失: 0.866280\n",
            "保存模型，准确率 0.718\n",
            "[009/200] 训练准确率: 0.693515 损失: 0.952244 | 验证准确率: 0.721540 损失: 0.853192\n",
            "保存模型，准确率 0.722\n",
            "[010/200] 训练准确率: 0.697713 损失: 0.935911 | 验证准确率: 0.724541 损失: 0.843046\n",
            "保存模型，准确率 0.725\n",
            "[011/200] 训练准确率: 0.701091 损失: 0.923791 | 验证准确率: 0.726675 损失: 0.832636\n",
            "保存模型，准确率 0.727\n",
            "[012/200] 训练准确率: 0.704990 损失: 0.911420 | 验证准确率: 0.730187 损失: 0.823018\n",
            "保存模型，准确率 0.730\n",
            "[013/200] 训练准确率: 0.707996 损失: 0.900305 | 验证准确率: 0.732614 损失: 0.815507\n",
            "保存模型，准确率 0.733\n",
            "[014/200] 训练准确率: 0.710207 损失: 0.890787 | 验证准确率: 0.734780 损失: 0.808228\n",
            "保存模型，准确率 0.735\n",
            "[015/200] 训练准确率: 0.713031 损失: 0.881565 | 验证准确率: 0.735971 损失: 0.802449\n",
            "保存模型，准确率 0.736\n",
            "[016/200] 训练准确率: 0.715589 损失: 0.872347 | 验证准确率: 0.737883 损失: 0.796171\n",
            "保存模型，准确率 0.738\n",
            "[017/200] 训练准确率: 0.717366 损失: 0.865038 | 验证准确率: 0.739032 损失: 0.790158\n",
            "保存模型，准确率 0.739\n",
            "[018/200] 训练准确率: 0.719498 损失: 0.857734 | 验证准确率: 0.741722 损失: 0.784815\n",
            "保存模型，准确率 0.742\n",
            "[019/200] 训练准确率: 0.721537 损失: 0.852050 | 验证准确率: 0.742629 损失: 0.779590\n",
            "保存模型，准确率 0.743\n",
            "[020/200] 训练准确率: 0.722938 损失: 0.844734 | 验证准确率: 0.743390 损失: 0.777539\n",
            "保存模型，准确率 0.743\n",
            "[021/200] 训练准确率: 0.725362 损失: 0.838891 | 验证准确率: 0.744961 损失: 0.771754\n",
            "保存模型，准确率 0.745\n",
            "[022/200] 训练准确率: 0.726113 损失: 0.833245 | 验证准确率: 0.745996 损失: 0.767292\n",
            "保存模型，准确率 0.746\n",
            "[023/200] 训练准确率: 0.728530 损失: 0.827524 | 验证准确率: 0.747561 损失: 0.762613\n",
            "保存模型，准确率 0.748\n",
            "[024/200] 训练准确率: 0.729850 损失: 0.822090 | 验证准确率: 0.748674 损失: 0.759353\n",
            "保存模型，准确率 0.749\n",
            "[025/200] 训练准确率: 0.731062 损失: 0.817679 | 验证准确率: 0.749013 损失: 0.758489\n",
            "保存模型，准确率 0.749\n",
            "[026/200] 训练准确率: 0.731668 损失: 0.813638 | 验证准确率: 0.749766 损失: 0.755000\n",
            "保存模型，准确率 0.750\n",
            "[027/200] 训练准确率: 0.733220 损失: 0.809033 | 验证准确率: 0.750247 损失: 0.752606\n",
            "保存模型，准确率 0.750\n",
            "[028/200] 训练准确率: 0.734406 损失: 0.805171 | 验证准确率: 0.752452 损失: 0.749211\n",
            "保存模型，准确率 0.752\n",
            "[029/200] 训练准确率: 0.735248 损失: 0.802410 | 验证准确率: 0.752406 损失: 0.747425\n",
            "[030/200] 训练准确率: 0.736575 损失: 0.796727 | 验证准确率: 0.753120 损失: 0.744662\n",
            "保存模型，准确率 0.753\n",
            "[031/200] 训练准确率: 0.738011 损失: 0.793699 | 验证准确率: 0.753672 损失: 0.741227\n",
            "保存模型，准确率 0.754\n",
            "[032/200] 训练准确率: 0.737937 损失: 0.791002 | 验证准确率: 0.755303 损失: 0.738575\n",
            "保存模型，准确率 0.755\n",
            "[033/200] 训练准确率: 0.739271 损失: 0.787670 | 验证准确率: 0.756220 损失: 0.735816\n",
            "保存模型，准确率 0.756\n",
            "[034/200] 训练准确率: 0.739852 损失: 0.785098 | 验证准确率: 0.756264 损失: 0.734628\n",
            "保存模型，准确率 0.756\n",
            "[035/200] 训练准确率: 0.740933 损失: 0.781337 | 验证准确率: 0.757103 损失: 0.732824\n",
            "保存模型，准确率 0.757\n",
            "[036/200] 训练准确率: 0.741570 损失: 0.779269 | 验证准确率: 0.757689 损失: 0.730576\n",
            "保存模型，准确率 0.758\n",
            "[037/200] 训练准确率: 0.742260 损失: 0.776869 | 验证准确率: 0.758225 损失: 0.728886\n",
            "保存模型，准确率 0.758\n",
            "[038/200] 训练准确率: 0.743975 损失: 0.773068 | 验证准确率: 0.758738 损失: 0.727533\n",
            "保存模型，准确率 0.759\n",
            "[039/200] 训练准确率: 0.744207 损失: 0.770291 | 验证准确率: 0.758682 损失: 0.726604\n",
            "[040/200] 训练准确率: 0.744897 损失: 0.769154 | 验证准确率: 0.759640 损失: 0.724031\n",
            "保存模型，准确率 0.760\n",
            "[041/200] 训练准确率: 0.745804 损失: 0.765751 | 验证准确率: 0.759960 损失: 0.723178\n",
            "保存模型，准确率 0.760\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[23], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 训练\u001b[39;00m\n\u001b[0;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m# 将模型设置为训练模式\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     17\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     18\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n",
            "File \u001b[1;32me:\\anaconda\\envs\\learntorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
            "File \u001b[1;32me:\\anaconda\\envs\\learntorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32me:\\anaconda\\envs\\learntorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32me:\\anaconda\\envs\\learntorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32me:\\anaconda\\envs\\learntorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:211\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    212\u001b[0m         collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[1;32me:\\anaconda\\envs\\learntorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[1;32me:\\anaconda\\envs\\learntorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
            "File \u001b[1;32me:\\anaconda\\envs\\learntorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 开始训练\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(NUM_EPOCH):\n",
        "    # 初始化训练和验证的准确率和损失\n",
        "    train_acc = 0.0\n",
        "    train_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    val_loss = 0.0\n",
        "    \n",
        "    # 早停相关变量，与交叉验证部分类似\n",
        "    patience_counter = 0 if 'patience_counter' not in locals() else patience_counter\n",
        "\n",
        "    # 训练\n",
        "    model.train() # 将模型设置为训练模式\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad() \n",
        "        outputs = model(inputs) \n",
        "        batch_loss = criterion(outputs, labels)\n",
        "        _, train_pred = torch.max(outputs, 1) # 获取具有最高概率的类的索引\n",
        "        batch_loss.backward() \n",
        "        optimizer.step() \n",
        "\n",
        "        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
        "        train_loss += batch_loss.item()\n",
        "\n",
        "    # 验证\n",
        "    if len(val_set) > 0:\n",
        "        model.eval() # 将模型设置为评估模式\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(val_loader):\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                batch_loss = criterion(outputs, labels) \n",
        "                _, val_pred = torch.max(outputs, 1) \n",
        "            \n",
        "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # 获取具有最高概率的类的索引\n",
        "                val_loss += batch_loss.item()\n",
        "\n",
        "            print('[{:03d}/{:03d}] 训练准确率: {:3.6f} 损失: {:3.6f} | 验证准确率: {:3.6f} 损失: {:3.6f}'.format(\n",
        "                epoch + 1, NUM_EPOCH, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n",
        "            ))\n",
        "\n",
        "            # 如果模型改进，在此轮保存检查点\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                torch.save(model.state_dict(), MODEL_PATH)\n",
        "                print('保存模型，准确率 {:.3f}'.format(best_acc/len(val_set)))\n",
        "    else:\n",
        "        print('[{:03d}/{:03d}] 训练准确率: {:3.6f} 损失: {:3.6f}'.format(\n",
        "            epoch + 1, NUM_EPOCH, train_acc/len(train_set), train_loss/len(train_loader)\n",
        "        ))\n",
        "\n",
        "# 如果不进行验证，保存最后一轮\n",
        "if len(val_set) == 0:\n",
        "    torch.save(model.state_dict(), MODEL_PATH)\n",
        "    print('在最后一轮保存模型')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hi7jTn3PX-m"
      },
      "source": [
        "## 测试"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfUECMFCn5VG"
      },
      "source": [
        "创建测试数据集，并从保存的检查点加载模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PKjtAScPWtr",
        "outputId": "8c17272b-536a-4692-a95f-a3292766c698"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 创建测试数据集\n",
        "test_set = TIMITDataset(test, None)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# 创建模型并从检查点加载权重\n",
        "model = Classifier().to(device)\n",
        "model.load_state_dict(torch.load(MODEL_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "940TtCCdoYd0"
      },
      "source": [
        "进行预测。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84HU5GGjPqR0"
      },
      "outputs": [],
      "source": [
        "predict = []\n",
        "model.eval() # 将模型设置为评估模式\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(test_loader):\n",
        "        inputs = data\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, test_pred = torch.max(outputs, 1) # 获取具有最高概率的类的索引\n",
        "\n",
        "        for y in test_pred.cpu().numpy():\n",
        "            predict.append(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWDf_C-omElb"
      },
      "source": [
        "将预测结果写入CSV文件。\n",
        "\n",
        "运行此代码块后，从左侧文件部分下载`prediction.csv`文件并提交到Kaggle。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuljYSPHcZir"
      },
      "outputs": [],
      "source": [
        "with open('prediction.csv', 'w') as f:\n",
        "    f.write('Id,Class\\n')\n",
        "    for i, y in enumerate(predict):\n",
        "        f.write('{},{}\\n'.format(i, y))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "SHARE MLSpring2021 - HW2-1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "learntorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
